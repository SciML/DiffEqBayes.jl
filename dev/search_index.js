var documenterSearchIndex = {"docs":
[{"location":"examples/#Bayesian-Inference-Examples","page":"Examples","title":"Bayesian Inference Examples","text":"","category":"section"},{"location":"examples/#Stan","page":"Examples","title":"Stan","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Like in the previous examples, we set up the Lotka-Volterra system and generate data.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"f1 = @ode_def begin\n  dx = a*x - b*x*y\n  dy = -c*y + d*x*y\nend a b c d\np = [1.5,1.0,3.0,1.0]\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(f1,u0,tspan,p)\nsol = solve(prob1,Tsit5())\nt = collect(range(1,stop=10,length=10))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Here we now give Stan an array of prior distributions for our parameters. Since the parameters of our differential equation must be positive, we utilize truncated Normal distributions to make sure that is satisfied in the result:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"priors = [truncated(Normal(1.5,0.1),0,2),truncated(Normal(1.0,0.1),0,1.5),\n          truncated(Normal(3.0,0.1),0,4),truncated(Normal(1.0,0.1),0,2)]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We then give these to the inference function.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bayesian_result = stan_inference(prob1,t,data,priors;\n                                 num_samples=100,num_warmup=500,\n                                 vars = (StanODEData(),InverseGamma(4,1)))","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"InverseGamma(4,1) is our starting estimation for the variance hyperparameter of the default Normal distribution. The result is a Mamba.jl chain object. We can pull out the parameter values via:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"theta1 = bayesian_result.chain_results[:,[\"theta.1\"],:]\ntheta2 = bayesian_result.chain_results[:,[\"theta.2\"],:]\ntheta3 = bayesian_result.chain_results[:,[\"theta.3\"],:]\ntheta4 = bayesian_result.chain_results[:,[\"theta.4\"],:]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"From these chains we can get our estimate for the parameters via:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"mean(theta1.value[:,:,1])","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"We can get more of a description via:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Mamba.describe(bayesian_result.chain_results)\n\n# Result\n\nIterations = 1:100\nThinning interval = 1\nChains = 1,2,3,4\nSamples per chain = 100\n\nEmpirical Posterior Estimates:\n                  Mean         SD        Naive SE        MCSE         ESS\n         lp__ -6.15472697 1.657551334 0.08287756670 0.18425029767  80.9314979\naccept_stat__  0.90165904 0.125913744 0.00629568721 0.02781181930  20.4968668\n   stepsize__  0.68014975 0.112183047 0.00560915237 0.06468790087   3.0075188\n  treedepth__  2.68750000 0.524911975 0.02624559875 0.10711170182  24.0159141\n n_leapfrog__  6.77000000 4.121841086 0.20609205428 0.18645821695 100.0000000\n  divergent__  0.00000000 0.000000000 0.00000000000 0.00000000000         NaN\n     energy__  9.12245750 2.518330231 0.12591651153 0.32894488320  58.6109941\n     sigma1.1  0.57164997 0.128579363 0.00642896816 0.00444242658 100.0000000\n     sigma1.2  0.58981422 0.131346442 0.00656732209 0.00397310122 100.0000000\n       theta1  1.50237077 0.008234095 0.00041170473 0.00025803930 100.0000000\n       theta2  0.99778276 0.009752574 0.00048762870 0.00009717115 100.0000000\n       theta3  3.00087782 0.009619775 0.00048098873 0.00020301023 100.0000000\n       theta4  0.99803569 0.008893244 0.00044466218 0.00040886528 100.0000000\n      theta.1  1.50237077 0.008234095 0.00041170473 0.00025803930 100.0000000\n      theta.2  0.99778276 0.009752574 0.00048762870 0.00009717115 100.0000000\n      theta.3  3.00087782 0.009619775 0.00048098873 0.00020301023 100.0000000\n      theta.4  0.99803569 0.008893244 0.00044466218 0.00040886528 100.0000000\n\nQuantiles:\n                  2.5%        25.0%      50.0%      75.0%       97.5%\n         lp__ -10.11994750 -7.0569000 -5.8086150 -4.96936500 -3.81514375\naccept_stat__   0.54808912  0.8624483  0.9472840  0.98695850  1.00000000\n   stepsize__   0.57975100  0.5813920  0.6440120  0.74276975  0.85282400\n  treedepth__   2.00000000  2.0000000  3.0000000  3.00000000  3.00000000\n n_leapfrog__   3.00000000  7.0000000  7.0000000  7.00000000 15.00000000\n  divergent__   0.00000000  0.0000000  0.0000000  0.00000000  0.00000000\n     energy__   5.54070300  7.2602200  8.7707000 10.74517500 14.91849500\n     sigma1.1   0.38135240  0.4740865  0.5533195  0.64092575  0.89713635\n     sigma1.2   0.39674703  0.4982615  0.5613655  0.66973025  0.88361407\n       theta1   1.48728600  1.4967650  1.5022750  1.50805500  1.51931475\n       theta2   0.97685115  0.9914630  0.9971435  1.00394250  1.01765575\n       theta3   2.98354100  2.9937575  3.0001450  3.00819000  3.02065950\n       theta4   0.97934128  0.9918495  0.9977415  1.00430750  1.01442975\n      theta.1   1.48728600  1.4967650  1.5022750  1.50805500  1.51931475\n      theta.2   0.97685115  0.9914630  0.9971435  1.00394250  1.01765575\n      theta.3   2.98354100  2.9937575  3.0001450  3.00819000  3.02065950\n      theta.4   0.97934128  0.9918495  0.9977415  1.00430750  1.01442975","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"More extensive information about the distributions is given by the plots:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"plot_chain(bayesian_result)","category":"page"},{"location":"examples/#Turing","page":"Examples","title":"Turing","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This case we will build off of the Stan example. Note that turing_inference does not require the use of the @ode_def macro like Stan does, but it will still work with macro-defined functions. Thus, using the same setup as before, we simply give the setup to:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bayesian_result = turing_inference(prob1,Tsit5(),t,data,priors;num_samples=500)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The result is a MCMCChains.jl chains object. The chain for the first parameter is then given by:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bayesian_result[\"theta[1]\"]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Summary statistics can be also be accessed:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using StatsBase\ndescribe(bayesian_result)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The chain can be analysed by the trace plots and other plots obtained by:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using StatsPlots\nplot(bayesian_result)","category":"page"},{"location":"examples/#DynamicHMC","page":"Examples","title":"DynamicHMC","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"We can use DynamicHMC.jl as the backend for sampling with the dynamic_inference function. It supports any DEProblem, priors can be passed as an array of Distributions.jl distributions, passing initial values is optional and in case where the user has a firm understanding of the domain the parameter values will lie in, transformations can be used to pass an array of constraints for the parameters as an array of Transformations.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bayesian_result_hmc = dynamichmc_inference(prob1, Tsit5(), t, data, [Normal(1.5, 1)], [bridge(ℝ, ℝ⁺, )])","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"A tuple with summary statistics and the chain values is returned. The chain for the ith parameter is given by:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bayesian_result_hmc[1][i]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For accessing the various summary statistics:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"DynamicHMC.NUTS_statistics(bayesian_result_dynamic[2])","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Some details about the NUTS sampler can be obtained from:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bayesian_result_dynamic[3]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"In case of dynamic_inference the trace plots for the ith parameter can be obtained by:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"plot(bayesian_result_hmc[1][i])","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For a better idea of the summary statistics and plotting you can take a look at the benchmarks.","category":"page"},{"location":"methods/#Bayesian-Methods","page":"Methods","title":"Bayesian Methods","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"The following methods require DiffEqBayes.jl:","category":"page"},{"location":"methods/","page":"Methods","title":"Methods","text":"]add DiffEqBayes\nusing DiffEqBayes","category":"page"},{"location":"methods/#stan_inference","page":"Methods","title":"stan_inference","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"stan_inference(prob::ODEProblem,t,data,priors = nothing;alg=:rk45,\n               num_samples=1000, num_warmups=1000, reltol=1e-3,\n               abstol=1e-6, maxiter=Int(1e5),likelihood=Normal,\n               vars=(StanODEData(),InverseGamma(2,3)))","category":"page"},{"location":"methods/","page":"Methods","title":"Methods","text":"stan_inference uses Stan.jl to perform the Bayesian inference. The Stan installation process is required to use this function. t is the array of time and data is the array where the first dimension (columns) corresponds to the array of system values. priors is an array of prior distributions for each parameter, specified via a Distributions.jl type. alg is a choice between :rk45 and :bdf, the two internal integrators of Stan. num_samples is the number of samples to take per chain, and num_warmups is the number of MCMC warmup steps. abstol and reltol are the keyword arguments for the internal integrator. likelihood is the likelihood distribution to use with the arguments from vars, and vars is a tuple of priors for the distributions of the likelihood hyperparameters. The special value StanODEData() in this tuple denotes the position that the ODE solution takes in the likelihood's parameter list.","category":"page"},{"location":"methods/#turing_inference","page":"Methods","title":"turing_inference","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"function turing_inference(prob::DiffEqBase.DEProblem,alg,t,data,priors;\n                              likelihood_dist_priors, likelihood, num_samples=1000,\n                              sampler = Turing.NUTS(num_samples, 0.65), syms, kwargs...)","category":"page"},{"location":"methods/","page":"Methods","title":"Methods","text":"turing_inference uses Turing.jl to perform its parameter inference. prob can be any DEProblem with a corresponding alg choice. t is the array of time points and data is the set of observations for the differential equation system at time point t[i] (or higher dimensional). priors is an array of prior distributions for each parameter, specified via a Distributions.jl type. num_samples is the number of samples per MCMC chain. The extra kwargs are given to the internal differential equation solver.","category":"page"},{"location":"methods/#dynamichmc_inference","page":"Methods","title":"dynamichmc_inference","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"dynamichmc_inference(prob::DEProblem,alg,t,data,priors,transformations;\n                      σ = 0.01,ϵ=0.001,initial=Float64[])","category":"page"},{"location":"methods/","page":"Methods","title":"Methods","text":"dynamichmc_inference uses DynamicHMC.jl to  perform the bayesian parameter estimation. prob can be any DEProblem, data is the set  of observations for our model which is to be used in the Bayesian Inference process. priors represent the  choice of prior distributions for the parameters to be determined, passed as an array of Distributions.jl distributions. t is the array of time points. transformations  is an array of Tranformations imposed for constraining the  parameter values to specific domains. initial values for the parameters can be passed, if not passed the means of the  priors are used. ϵ can be used as a kwarg to pass the initial step size for the NUTS algorithm.","category":"page"},{"location":"methods/#abc_inference","page":"Methods","title":"abc_inference","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"abc_inference(prob::DEProblem, alg, t, data, priors; ϵ=0.001,\n     distancefunction = euclidean, ABCalgorithm = ABCSMC, progress = false,\n     num_samples = 500, maxiterations = 10^5, kwargs...)","category":"page"},{"location":"methods/","page":"Methods","title":"Methods","text":"abc_inference uses ApproxBayes.jl which uses Approximate Bayesian Computation (ABC) to perform its parameter inference. prob can be any DEProblem with a corresponding alg choice. t is the array of time points and data[:,i] is the set of observations for the differential equation system at time point t[i] (or higher dimensional). priors is an array of prior distributions for each parameter, specified via a Distributions.jl type. num_samples is the number of posterior samples. ϵ is the target distance between the data and simulated data. distancefunction is a distance metric specified from the Distances.jl package, the default is euclidean. ABCalgorithm is the ABC algorithm to use, options are ABCSMC or ABCRejection from ApproxBayes.jl, the default is the former which is more efficient. maxiterations is the maximum number of iterations before the algorithm terminates. The extra kwargs are given to the internal differential equation solver.","category":"page"},{"location":"#DiffEqBayes.jl:-Bayesian-Parameter-Estimation-for-Differential-Equations","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"","category":"section"},{"location":"","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"This repository is a set of extension functionality for estimating the parameters of differential equations using Bayesian methods. It allows the choice of using CmdStan.jl, Turing.jl, DynamicHMC.jl and ApproxBayes.jl to perform a Bayesian estimation of a differential equation problem specified via the DifferentialEquations.jl interface.","category":"page"},{"location":"#Installation","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"Installation","text":"","category":"section"},{"location":"","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"To install DiffEqBayes.jl, use the Julia package manager:","category":"page"},{"location":"","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"using Pkg\nPkg.add(\"DiffEqBayes\")","category":"page"},{"location":"#Contributing","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"Contributing","text":"","category":"section"},{"location":"","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"}]
}
